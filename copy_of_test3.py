# -*- coding: utf-8 -*-
"""Copy of test3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BbnEQHl_X6G-5sLIvXSyAbj9xctP0rYu
"""

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
from __future__ import unicode_literals
import torch
from torch.jit import script,trace
import torch.nn as nn
from torch import optim
import torch.nn.functional as F
import csv,random,re,os,unicodedata
import codecs
from io import open
import itertools
import math
from Voc import PAD_token,SOS_token,EOS_token,Voc
from Seq2seq import EncoderRNN,LuongAttnDecoderRNN
from Loss import maskNLLLoss
from Trainer import SeqTrainer
from Seq2seqModel import Seq2seq
from NLPTOperator import NLPOperator
from Evaluator import InputBotEvaluator
from functions import *
from DataReader import DataReader
import json
from ParameterLoader import ParameterLoader

USE_CUDA = torch.cuda.is_available()
device = torch.device("cuda" if USE_CUDA else "cpu")
dataReader = DataReader(Voc)
save_dir = os.path.join("data", "save")
# 读入文件系列,主要是给出pair和voc
# pair的结果是格式化过的一对字符串形成的列表
voc, pairs = dataReader.loadPrepareData(corpus_name,datafile)

# hyperparameter========
with open("hyperparameter.json","r") as f:
    config = json.load(f)
model_name = config["model"]["name"]
attn_model = config["model"]["attn"]

hidden_size = config["hyper"]["hidden_size"]
encoder_n_layers = config["hyper"]["encoder_n_layers"]
decoder_n_layers = config["hyper"]["decoder_n_layers"]
dropout = config["hyper"]["dropout"]
batch_size = config["hyper"]["batch_size"]
MAX_LENGTH = config["hyper"]["MAX_LENGTH"]
eps = config["hyper"]["eps"]

loading = config["load"]["load"]
loadFilename = config["load"]["file"]
iters = config["load"]["iter"]

output_size = voc.num_words
checkpoint = None
loader = ParameterLoader(save_dir, model_name, corpus_name)
# ===============
if loading:
    model_sd,encoder_optimizer_sd,decoder_optimizer_sd,voc_dict,iteration = loader.load(loadFilename,checkpoint_iter)
    voc.__dict__ = voc_dict
    checkpoint = iteration
print('Building encoder and decoder...')

embedding = nn.Embedding(output_size,hidden_size)
encoder = EncoderRNN(hidden_size,embedding,encoder_n_layers,dropout).to(device)
decoder = LuongAttnDecoderRNN(attn_model,embedding,hidden_size,output_size,decoder_n_layers,dropout).to(device)

print('Ready!')
#loadFilename = os.path.join(save_dir, model_name, corpus_name,
#                            '{}_checkpoint.tar'.format(checkpoint_iter))
seq2seq = Seq2seq(encoder,decoder,device,maskNLLLoss)
if loading:
    seq2seq.load_state_dict(model_sd)
seq2seq.to(device)
# hyper==========================
clip = config["hyper"]["clip"]
teacher_forcing_ratio = config["hyper"]["teacher_forcing_ratio"]
learning_rate = config["hyper"]["learning_rate"]
decoder_learning_ratio = config["hyper"]["decoder_learning_ratio"]
n_iteration = config["hyper"]["n_iteration"]
print_every = config["hyper"]["print_every"]
save_every = config["hyper"]["save_every"]
# =================================
encoder.train()
decoder.train()
print('Building optimizers...')
encoder_optimizer = optim.Adam(encoder.parameters(),lr=learning_rate,eps=eps)
decoder_optimizer = optim.Adam(decoder.parameters(),lr=learning_rate * decoder_learning_ratio,eps=eps)
if loading:
    encoder_optimizer.load_state_dict(encoder_optimizer_sd)
    decoder_optimizer.load_state_dict(decoder_optimizer_sd)
for state in encoder_optimizer.state.values():
    for k,v in state.items():
        if isinstance(v,torch.Tensor):
            state[k] = v.cuda()
for state in decoder_optimizer.state.values():
    for k,v in state.items():
        if isinstance(v,torch.Tensor):
            state[k] = v.cuda()
print('Start training!')
trainer = SeqTrainer(seq2seq,encoder_optimizer,decoder_optimizer,device)
operator = NLPOperator(trainer, voc,loader ,print_every, save_every,clip,checkpoint)
evaluator = InputBotEvaluator(seq2seq,voc,device,MAX_LENGTH)

operator.train(pairs,n_iteration,batch_size)



# Begin chatting (uncomment and run the following line to begin)
evaluateInput(seq2seq, voc,evaluator)

"""# New Section"""